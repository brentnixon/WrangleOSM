{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling Project - Open Street Map\n",
    "__Brent Nixon, Dec. 2017__\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "* <a href='#Introduction'>Introduction</a>\n",
    "* <a href='#Extracting Data from OSM'>Extracting Data from OSM</a>\n",
    "* <a href='#Overview of Data and Checking for Problems'>Overview of Data and Checking for Problems</a>\n",
    "* <a href='#Data Processing and Cleaning'>Data Processing and Cleaning</a>\n",
    "* <a href='#Loading Data into SQL'>Loading Data into SQL</a>\n",
    "* <a href='#Exploration of Data with SQL'>Exploration of Data with SQL</a>\n",
    "* <a href='#Worthy Options for Further Cleaning'>Worthy Options for Further Cleaning</a>\n",
    "\n",
    "Reference:\n",
    "http://sebastianraschka.com/Articles/2014_ipython_internal_links.html#top"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction <a id='Introduction'></a>\n",
    "\n",
    ">In this project, I will examine a section of the Open Street Map (OSM). To do this, I will extract the data from OSM, process the data programatically, look for issues in the data (such as cleanliness, uniformity, and validity), load the data into a local SQL database, and then explore the data using SQL to get an overview of the area.\n",
    "\n",
    ">I chose to examine OSM data from the southeast part of Charlotte, North Carolina, which is where I grew up. \n",
    "\n",
    ">Navigate to this link to examine the bounding box describing the area I worked with: http://www.openstreetmap.org/relation/177415#map=11/35.2033/-80.8401\n",
    "\n",
    ">Charlotte is an interesting city because it has a wide variety of urbanism. There is a dense urban core with high-rises, semi-urban mixed residential and commercial areas, sub-urban residential areas, and low-density sprawl. For a mapping project, there would certainly be a variety of features and idiosyncrasies to sort out. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">This cell sets the path for the Jupyter notebook so it knows where to find the supporting files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ways_tags.csv', 'Charlotte_AOI.xml', 'nodes_tags.csv', '.DS_Store', 'nodes.csv', 'charlotte_osm.db', 'Wrangle_OSM_BNixon.ipynb', '__pycache__', 'data_wrangling_schema.sql', 'Charlotte_sample.xml', 'ways.csv', '.ipynb_checkpoints', 'SQL_code_for_charlotte_osm_db.sql', 'ways_nodes.csv', 'clt_osm_db_Keys.txt', 'schema.py']\n"
     ]
    }
   ],
   "source": [
    "# set working directory\n",
    "import os\n",
    "PATH= \"/Users/brentan/Documents/DAND/Projects/Data_Wrangling_Project/Wrangle_OSM_BNixon\"\n",
    "\n",
    "os.chdir(PATH)\n",
    "print(os.listdir('.'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Data from OSM <a id='Extracting Data from OSM'></a>\n",
    ">The first step in this process is to obtain the data from OSM, we need to make an HTTP request using the Overpass API. The function below, \"get_XML_data\" takes an OSM url showing the location of interest, and writes the streaming requests object line by line to a file. \n",
    "\n",
    "*The code below is adapted from the Udacity case study helper.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## code for getting XML data from OSM using the Overpass API\n",
    "import requests\n",
    "def get_XML_data(URL, FILENAME): \n",
    "    \n",
    "    # make the request but have it stream instead of reading all data into memory at once\n",
    "    r = requests.get(URL, stream=True)\n",
    "    \n",
    "    try: \n",
    "        # print the URL to debug in event that error gets thrown \n",
    "        print(\"Request URL:\",r.url)\n",
    "\n",
    "        # Throw an error for bad status codes\n",
    "        r.raise_for_status()\n",
    "        \n",
    "        # use iter_lines to parse each line one by one\n",
    "        events = r.iter_lines() \n",
    "        \n",
    "        # write each line, line by line, into a writable file\n",
    "        with open (FILENAME, 'w') as f:\n",
    "            for line in events:\n",
    "                f.write(line.decode('utf-8'))\n",
    "                f.write(\"\\n\") # this is necessary to maintain the formatting in the file\n",
    "                \n",
    "        # success messages        \n",
    "        print(\"File write was success!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print (e)\n",
    "        \n",
    "    finally: \n",
    "        r.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">The code below will use the \"get_XML_data\" function to pull the data from OSM for the area of interest in Charlotte. For our final work, we want to work with at least a 50mb file, but to start out, I will explore the data and test my code using a 5-10mb file. The code below will create a sample file that takes elements from throughout the project file to give an accurate representation of issues that will be found in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request URL: http://overpass-api.de/api/map?bbox=-80.8934,35.1157,-80.7368,35.2310\n",
      "File write was success!\n"
     ]
    }
   ],
   "source": [
    "## Code for sample acquisition\n",
    "SAMPLE_URL = \"http://overpass-api.de/api/map?bbox=-80.8676,35.1993,-80.8099,35.2218\"\n",
    "SAMPLE_FILENAME = \"Charlotte_sample.xml\"\n",
    "# get_XML_data(SAMPLE_URL, SAMPLE_FILENAME)\n",
    "\n",
    "# Code for 50mb size dataset\n",
    "URL = \"http://overpass-api.de/api/map?bbox=-80.8934,35.1157,-80.7368,35.2310\"\n",
    "FILENAME = \"Charlotte_AOI.xml\"\n",
    "# get_XML_data(URL, FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of Data and Checking for Problems <a id='Overview of Data and Checking for Problems'></a>\n",
    ">Now that I have a sample XML file with data from Charlotte, I want to get a rough overview of the data using the way and node tags. This will help me have a working understanding of what my code is checking and what kind of data cleaning I might need to do. \n",
    "\n",
    ">The code below will iterate through the XML file finding tags. If a tag name is new, it will add that name to a dictionary. Otherwise, it will add one to the count for that tag type. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bounds': 1,\n",
      " 'member': 10600,\n",
      " 'meta': 1,\n",
      " 'nd': 306987,\n",
      " 'node': 270362,\n",
      " 'note': 1,\n",
      " 'osm': 1,\n",
      " 'relation': 171,\n",
      " 'tag': 77094,\n",
      " 'way': 31370}\n"
     ]
    }
   ],
   "source": [
    "## code block to explore element type and counts \n",
    "import xml.etree.cElementTree as ET  \n",
    "import pprint\n",
    "\n",
    "# function to list what element names are in the XML file and give their counts\n",
    "def count_tags(filename):\n",
    "    things = {}\n",
    "    for _, val in ET.iterparse(filename):\n",
    "        if val.tag in things:\n",
    "            things[val.tag] +=1\n",
    "        else:\n",
    "            things[val.tag] = 1\n",
    "    return things\n",
    "\n",
    "# execute function on XML file\n",
    "pprint.pprint(count_tags(\"Charlotte_AOI.xml\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overview of Data Structure\n",
    "\n",
    ">From the above code, we can see that there are a variety of different tags. The primary tags, however, are nd, node, tag, member, way, and relation. \n",
    "\n",
    ">From the OSM documentation, we see that the basic structure of OSM data is comprised of nodes, ways, and relations. Nodes are points on the map, ways are an ordered list of connected nodes (describing things like roads, rivers, etc...), and relations are composed of related members (nodes and ways). \n",
    "\n",
    ">Tag elements are sub-elements of each way, node, and relation, whose attributes provide further information that describe the way, node, or relation. Nd elements are sub-elements of ways, which reference the node elements that compose the way. \n",
    "\n",
    ">The last few elements, bounds,meta, note, and osm are all part of the header information of the XML document. The bounds tag contains coordinates showing the map area that the data is from. \n",
    "\n",
    ">*Reference:*\n",
    "\n",
    ">*http://wiki.openstreetmap.org/wiki/Map_Features*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Checking for Compound Values and Problem Characters\n",
    ">In some of the tags, there are attributes that have values containing two pieces of information in one text string, e.g. \"addr:street\" instead of \"address\" and \"street.\" When we bring that information into a SQL databases, it would be preferable to have those two pieces of information separated. \n",
    "\n",
    ">In the code below, I will use regular expressions to identify elements whose \"k\" attributes have a structure with colons as specified above, as well as attributes which do not and attributes which have problematic characters like #, ?, etc... This will help me isolate and correct those issues. \n",
    "\n",
    ">*The code below is adapted from the Udacity case study helper.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lower': 51567, 'lower_colon': 23439, 'other': 2088, 'problemchars': 0}\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "## code block to identify and count attribute keys with colon structure or problematic characters\n",
    "\n",
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "import re\n",
    "\n",
    "# define regular expressions for compiling\n",
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "# function to give a count of each of the four tag categories\n",
    "def count_key_type(element, keys): \n",
    "    if element.tag == \"tag\":\n",
    "        if lower.search(element.get('k')):\n",
    "            keys[\"lower\"] += 1\n",
    "        elif lower_colon.search(element.get('k')):\n",
    "            keys[\"lower_colon\"] += 1\n",
    "        elif problemchars.search(element.get('k')):\n",
    "            keys[\"problemchars\"] += 1\n",
    "        else: \n",
    "            keys[\"other\"] += 1\n",
    "    pass\n",
    "    return keys\n",
    "\n",
    "# function to iteratively process lines in XML file and yield dictionary with count of key types\n",
    "def process_map(filename):\n",
    "    keys = {\"lower\": 0, \"lower_colon\": 0, \"problemchars\": 0, \"other\": 0}\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        keys = count_key_type(element, keys)\n",
    "    return keys\n",
    "\n",
    "# execute process_map on XML file\n",
    "keys = process_map('Charlotte_AOI.xml')\n",
    "pprint.pprint(keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Running the code above shows there are:\n",
    ">* 51567 \"k\" attributes that are lower case letters and do not have any colons or problematic characters, there are \n",
    ">* 23439 \"k\" attributes with the colon structure\n",
    ">* 0 \"k\" attributes with problematic characters\n",
    ">* 2088 \"k\" attributes that don't fit into either of those three categories."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examining Different Tag Types\n",
    ">Now I'm curious what the values in each of those categories look like. I will modify the above code to add each type of attribute value to a list. Then I can print a sample of those values to get a better idea of what they are. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample of 'lower' values:\n",
      "['ele',\n",
      " 'highway',\n",
      " 'highway',\n",
      " 'lanes',\n",
      " 'highway',\n",
      " 'highway',\n",
      " 'source',\n",
      " 'bicycle',\n",
      " 'building',\n",
      " 'width',\n",
      " 'lanes',\n",
      " 'building',\n",
      " 'building',\n",
      " 'building',\n",
      " 'height',\n",
      " 'building',\n",
      " 'highway',\n",
      " 'height',\n",
      " 'building',\n",
      " 'building',\n",
      " 'building',\n",
      " 'access',\n",
      " 'building',\n",
      " 'service',\n",
      " 'surface']\n",
      "\n",
      " sample of 'lower_colon' values:\n",
      "['gnis:id',\n",
      " 'addr:state',\n",
      " 'tiger:county',\n",
      " 'tiger:name_base',\n",
      " 'tiger:cfcc',\n",
      " 'tiger:reviewed',\n",
      " 'tiger:county',\n",
      " 'tiger:reviewed',\n",
      " 'tiger:cfcc',\n",
      " 'tiger:zip_left',\n",
      " 'tiger:reviewed',\n",
      " 'tiger:name_base',\n",
      " 'tiger:name_base',\n",
      " 'tiger:reviewed',\n",
      " 'tiger:zip_left',\n",
      " 'tiger:zip_left',\n",
      " 'tiger:county',\n",
      " 'tiger:reviewed',\n",
      " 'hgv:national_network',\n",
      " 'tiger:zip_right',\n",
      " 'tiger:county',\n",
      " 'tiger:reviewed',\n",
      " 'tiger:name_type',\n",
      " 'tiger:name_base']\n",
      "\n",
      " sample of 'other' values:\n",
      "['gnis:Class',\n",
      " 'gnis:Class',\n",
      " 'gnis:Class',\n",
      " 'gnis:Class',\n",
      " 'tiger:name_base_1',\n",
      " 'name_1',\n",
      " 'HFCS',\n",
      " 'tiger:name_base_2',\n",
      " 'NCOS:RIV_BASIN',\n",
      " 'NCOS:RIV_BASIN',\n",
      " 'NCOS:SPOPL',\n",
      " 'NCOS:RIV_BASIN',\n",
      " 'tiger:name_base_2',\n",
      " 'tiger:name_base_2',\n",
      " 'tiger:name_base_1',\n",
      " 'tiger:name_base_1',\n",
      " 'HFCS',\n",
      " 'name_1']\n"
     ]
    }
   ],
   "source": [
    "## code block to identify different types of node and way tag key-attribute values, as well as \n",
    "## identifying colon-structured values and problem characters\n",
    "\n",
    "import xml.etree.cElementTree as ET\n",
    "import pprint\n",
    "import re\n",
    "\n",
    "# define regular expressions for compiling\n",
    "lower = re.compile(r'^([a-z]|_)*$')\n",
    "lower_colon = re.compile(r'^([a-z]|_)*:([a-z]|_)*$')\n",
    "problemchars = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "# function to identify values with colons and problem characters\n",
    "def get_key_type(element, key_vals): \n",
    "    if element.tag == \"tag\":\n",
    "        if lower.search(element.get('k')):\n",
    "            key_vals[\"lower\"].append(element.get('k'))\n",
    "        elif lower_colon.search(element.get('k')):\n",
    "            key_vals[\"lower_colon\"].append(element.get('k'))\n",
    "        elif problemchars.search(element.get('k')):\n",
    "            key_vals[\"problemchars\"].append(element.get('k'))\n",
    "        else: \n",
    "            key_vals[\"other\"].append(element.get('k'))\n",
    "    pass\n",
    "    return key_vals\n",
    "\n",
    "# function to iterate through each line of XML file and yield dictionary of lists with values of each type\n",
    "def process_map(filename):\n",
    "    key_vals = {\"lower\": [], \"lower_colon\": [], \"problemchars\": [], \"other\": []}\n",
    "    for _, element in ET.iterparse(filename):\n",
    "        key_vals = get_key_type(element, key_vals)\n",
    "    return key_vals\n",
    "\n",
    "# execute process_map function on XML file\n",
    "key_vals = process_map('Charlotte_AOI.xml')\n",
    "\n",
    "# print out different types of values\n",
    "print(\"sample of 'lower' values:\")\n",
    "pprint.pprint(key_vals[\"lower\"][0::2100])\n",
    "print(\"\\n\", \"sample of 'lower_colon' values:\")\n",
    "pprint.pprint(key_vals[\"lower_colon\"][0::1000])\n",
    "print(\"\\n\", \"sample of 'other' values:\")\n",
    "pprint.pprint(key_vals[\"other\"][0::120])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">From the above experiment, here are a few take-aways:\n",
    "\n",
    ">* The tags in the \"lower\" category do not appear to have any problems. \n",
    "\n",
    "\n",
    ">* In the \"other\" category, there are some attributes that were flagged because they weren't lower case. From the sample, it appears that these represent different acronyms, which are reasonable to be uppercase. \n",
    "\n",
    "\n",
    ">* In the \"lower_colon\" category, there are some values that I want to mess with, and some that I don't! Quite a few values are from US Census Tiger dataset, and plenty from the USGS Geographic Names Information System (GNIS). These values contain useful information, and could be used to create rich units of address information. \n",
    "\n",
    ">*References:*\n",
    ">* *http://wiki.openstreetmap.org/wiki/TIGER*\n",
    ">* *http://wiki.openstreetmap.org/wiki/USGS_GNIS*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " #### Getting an Overview of Street Information\n",
    ">Before cleaning, I first need to see what street types are present in the data. The code below will iterate through the sample, looking for any tags with attributes containing street information. This will help me see what potential issues there are in terms of inconsistent street naming or other data quality issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## code block to collect all the values with street information, and to isolate all values with problematic \n",
    "## street information\n",
    "import xml.etree.cElementTree as ET\n",
    "from collections import defaultdict, OrderedDict\n",
    "import re\n",
    "import pprint\n",
    "\n",
    "# define regular expression to identify street types\n",
    "osmfile = \"Charlotte_AOI.xml\"\n",
    "street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "\n",
    "expected = [\"Street\", \"Avenue\", \"Boulevard\", \"Drive\", \"Court\", \"Place\", \"Square\", \"Lane\", \"Road\", \n",
    "            \"Trail\", \"Parkway\", \"Commons\"]\n",
    "\n",
    "# dictionary of desired mapping to use when updating street types\n",
    "mapping = { \"St\": \"Street\",\n",
    "            \"St.\": \"Street\",\n",
    "            \"Ave\": \"Avenue\",\n",
    "            \"Ave.\": \"Avenue\",\n",
    "            \"Blvd\": \"Boulevard\",\n",
    "            \"Blvd.\": \"Boulevard\",\n",
    "            \"Dr\": \"Drive\",\n",
    "            \"Dr.\": \"Drive\",\n",
    "            \"Ext\": \"Extension\",\n",
    "            \"Ext.\": \"Extension\",\n",
    "            \"Ct\": \"Court\",\n",
    "            \"Ct.\": \"Court\",\n",
    "            \"Pl\": \"Place\",\n",
    "            \"Pl.\": \"Place\",\n",
    "            \"Sq\": \"Square\",\n",
    "            \"Sq.\": \"Square\",\n",
    "            \"Ln\": \"Lane\",\n",
    "            \"Ln.\": \"Lane\",\n",
    "            \"Rd\": \"Road\",\n",
    "            \"Rd.\": \"Road\",\n",
    "            \"Trl\": \"Trail\",\n",
    "            \"Trl.\": \"Trail\",\n",
    "            \"Pkwy\": \"Parkway\",\n",
    "            \"Pkwy.\": \"Parkway\",\n",
    "            \"Cmns\": \"Commons\",\n",
    "            \"Cmns.\":\"Commons\"\n",
    "            }\n",
    "\n",
    "# function to identify a 'k' attribute with street information\n",
    "def is_street_name(elem):     \n",
    "    return (elem.attrib['k'] == \"addr:street\")\n",
    "\n",
    "################\n",
    "## block to collect any values with street information\n",
    "\n",
    "# function to identify a record with any street information and add it to a dictionary\n",
    "def get_any_street_type(all_streets, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        all_streets[street_type].add(street_name)\n",
    "\n",
    "# function to iterate through XML file, collecting a set of all unique values with street information \n",
    "def get_all_streets(osm_file):\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    all_streets = defaultdict(set)\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)): \n",
    "        if elem.tag == \"node\" or elem.tag == \"way\" or elem.tag == \"relation\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_street_name(tag):\n",
    "                    get_any_street_type(all_streets, tag.attrib['v'])\n",
    "    osm_file.close()\n",
    "    return all_streets\n",
    "\n",
    "##########################\n",
    "## block to isolate problematic street information\n",
    "\n",
    "# function to identify a record with unexpected street information and add it to a dictionary\n",
    "def audit_street_type(street_types, street_name):\n",
    "    m = street_type_re.search(street_name)\n",
    "    if m:\n",
    "        street_type = m.group()\n",
    "        if street_type not in expected:\n",
    "            street_types[street_type].add(street_name)\n",
    "            \n",
    "# function to iterate through XML file, collecting a set of unique values with unexpected street information \n",
    "def audit(osm_file):\n",
    "    osm_file = open(osmfile, \"r\")\n",
    "    street_types = defaultdict(set)\n",
    "    for event, elem in ET.iterparse(osm_file, events=(\"start\",)): \n",
    "        if elem.tag == \"node\" or elem.tag == \"way\" or elem.tag == \"relation\":\n",
    "            for tag in elem.iter(\"tag\"):\n",
    "                if is_street_name(tag):\n",
    "                    audit_street_type(street_types, tag.attrib['v'])\n",
    "    osm_file.close()\n",
    "    return street_types\n",
    "\n",
    "# function to take a problematic street type and update it to the correct type using the mapping dictionary\n",
    "def update_name(name, mapping):\n",
    "    street_bits = name.split(\" \")\n",
    "    if street_bits[-1] in mapping:\n",
    "        street_bits[-1] = mapping[street_bits[-1]]\n",
    "    return \" \".join(street_bits)\n",
    "\n",
    "############\n",
    "# code block executing the above functions\n",
    "\n",
    "# # execute and print results of get_all_streets on the XML file, shows all values with street information\n",
    "# pprint.pprint(dict(get_all_streets(osmfile)))\n",
    "\n",
    "# # execute and print results of audit on the XML file, collects and prints problematic street information\n",
    "# problematic_street_types = audit(osmfile)\n",
    "# pprint.pprint(dict(audit(osmfile)))\n",
    "\n",
    "# # execute the update_name function and print the mapping of old street name to new, updated street name\n",
    "# for st_type, ways in problematic_street_types.items(): \n",
    "#     for name in ways:\n",
    "#         better_name = update_name(name, mapping)\n",
    "#         print (name, \"=>\", better_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'4': {'Abbey Pl #4'},\n",
      " '4b': {'Sardis Road North Suite 4b'},\n",
      " 'Ardsley': {'Ardsley'},\n",
      " 'Ave': {'Winthrop Ave'},\n",
      " 'Ave.': {'2001 Selwyn Ave.'},\n",
      " 'Avenue': {'Baldwin Avenue',\n",
      "            'Central Avenue',\n",
      "            'Charlottetowne Avenue',\n",
      "            'Commonwealth Avenue',\n",
      "            'Craig Avenue',\n",
      "            'Draper Avenue',\n",
      "            'East Kingston Avenue',\n",
      "            'East Park Avenue',\n",
      "            'Hillside Avenue',\n",
      "            'Kenwood Avenue',\n",
      "            'Louise Avenue',\n",
      "            'Magnolia Avenue',\n",
      "            'Metropolitan Avenue',\n",
      "            'North Old Woodward Avenue',\n",
      "            'Pecan Avenue',\n",
      "            'Radcliffe Avenue',\n",
      "            'Roswell Avenue',\n",
      "            'Scott Avenue',\n",
      "            'Seigle Avenue',\n",
      "            'Selwyn Avenue',\n",
      "            'Skyland Avenue',\n",
      "            'Toomey Avenue',\n",
      "            'Union Pacific Avenue',\n",
      "            'Vail Avenue',\n",
      "            'West Kingston Avenue',\n",
      "            'West Park Avenue',\n",
      "            'West Tremont Avenue'},\n",
      " 'Blvd': {'Blythe Blvd', 'South Blvd', 'East Blvd'},\n",
      " 'Boulevard': {'Blythe Boulevard',\n",
      "               'Carnegie Boulevard',\n",
      "               'East Boulevard',\n",
      "               'East Independence Boulevard',\n",
      "               'East Martin Luther King Boulevard',\n",
      "               'South Boulevard',\n",
      "               'Stuart Andrew Boulevard',\n",
      "               'West Carson Boulevard'},\n",
      " 'Dr': {'Montford Dr'},\n",
      " 'Drive': {'Ashland Avenue and Kensington Drive',\n",
      "           'Barringer Drive',\n",
      "           'Boyce Road Park Drive',\n",
      "           'Flynwood Drive',\n",
      "           'Haven Drive',\n",
      "           'Latrobe Drive',\n",
      "           'Mattingwood Drive',\n",
      "           'Middleton Drive',\n",
      "           'Montford Drive',\n",
      "           'Morehead Medical Drive',\n",
      "           'Sablewood Drive',\n",
      "           'Seventy-Seven Center Drive',\n",
      "           'South Kings Drive',\n",
      "           'Southside Drive',\n",
      "           'West Drive'},\n",
      " 'Ext': {'West 4th Street Ext'},\n",
      " 'Lane': {'Antlers Lane',\n",
      "          'Arena Lane',\n",
      "          'Ashley Park Lane',\n",
      "          'Cherrycrest Lane',\n",
      "          'Hawthorne Lane',\n",
      "          'Runnymede Lane'},\n",
      " 'North': {'Sardis Road North'},\n",
      " 'Parkway': {'Cameron Valley Parkway'},\n",
      " 'Pl': {'Harrisonwoods Pl'},\n",
      " 'Place': {'Harding Place', 'Hempstead Place', 'Dorchester Place'},\n",
      " 'Plaza': {'The Plaza'},\n",
      " 'Rd': {'Old Nations Ford Rd', 'Monroe Rd', 'Albemarle Rd'},\n",
      " 'Road': {'Applegate Road',\n",
      "          'Billingsley Road',\n",
      "          'Camden Road',\n",
      "          'Carmel Road',\n",
      "          'Cherokee Road',\n",
      "          'Clanton Road',\n",
      "          'Colony Road',\n",
      "          'Colville Road',\n",
      "          'East Woodlawn Road',\n",
      "          'Elmhurst Road',\n",
      "          'Gleneagles Road',\n",
      "          'Greenwich Road',\n",
      "          'Griffith Road',\n",
      "          'Marsh Road',\n",
      "          'Monroe Road',\n",
      "          'Norland Road',\n",
      "          'North Sharon Amity Road',\n",
      "          'North Wendover Road',\n",
      "          'Old Pineville Road',\n",
      "          'Park Road',\n",
      "          'Providence Road',\n",
      "          'Quail Hollow Road',\n",
      "          'Queens Road',\n",
      "          'Randolph Road',\n",
      "          'Reddman Road',\n",
      "          'Sedgefield Road',\n",
      "          'Simsbury Road',\n",
      "          'South Sharon Amity Road',\n",
      "          'Spring Valley Road',\n",
      "          'Tyvola Road',\n",
      "          'Walton Road',\n",
      "          'Water Oak Road',\n",
      "          'Yancey Road'},\n",
      " 'St': {'W Hill St'},\n",
      " 'Street': {'Beal Street',\n",
      "            'Distribution Street',\n",
      "            'East 3rd Street',\n",
      "            'East 4th Street',\n",
      "            'East 5th Street',\n",
      "            'East 7th Street',\n",
      "            'East 9th Street',\n",
      "            'East Bland Street',\n",
      "            'East Morehead Street',\n",
      "            'East Trade Street',\n",
      "            'Griffith Street',\n",
      "            'Hawkins Street',\n",
      "            'Leroy Street',\n",
      "            'Lincoln Street',\n",
      "            'May Street',\n",
      "            'North Brevard Street',\n",
      "            'North Caldwell Street',\n",
      "            'North Church Street',\n",
      "            'North College Street',\n",
      "            'North Davidson Street',\n",
      "            'North Tryon Street',\n",
      "            'South Brevard Street',\n",
      "            'South Caldwell Street',\n",
      "            'South Church Street',\n",
      "            'South College Street',\n",
      "            'South Davidson Street',\n",
      "            'South McDowell Street',\n",
      "            'South Mint Street',\n",
      "            'South Poplar Street',\n",
      "            'South Tryon Street',\n",
      "            'Spruce Street',\n",
      "            'West 5th Street',\n",
      "            'West Bland Street',\n",
      "            'West Morehead Street',\n",
      "            'West Palmer Street',\n",
      "            'West Trade Street'},\n",
      " 'Way': {'Iverson Way'}}\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(dict(get_all_streets(osmfile)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Overall, the streets and their abbreviations look correctly spelled and formulated. Other than a few abbreviations that I'd like to see full words for (i.e. Blvd. to Boulevard), there are really no issues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Now I want to isolate values with problematic street types to see what kind of issues there are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'4': {'Abbey Pl #4'},\n",
      " '4b': {'Sardis Road North Suite 4b'},\n",
      " 'Ardsley': {'Ardsley'},\n",
      " 'Ave': {'Winthrop Ave'},\n",
      " 'Ave.': {'2001 Selwyn Ave.'},\n",
      " 'Blvd': {'Blythe Blvd', 'South Blvd', 'East Blvd'},\n",
      " 'Dr': {'Montford Dr'},\n",
      " 'Ext': {'West 4th Street Ext'},\n",
      " 'North': {'Sardis Road North'},\n",
      " 'Pl': {'Harrisonwoods Pl'},\n",
      " 'Plaza': {'The Plaza'},\n",
      " 'Rd': {'Old Nations Ford Rd', 'Monroe Rd', 'Albemarle Rd'},\n",
      " 'St': {'W Hill St'},\n",
      " 'Way': {'Iverson Way'}}\n"
     ]
    }
   ],
   "source": [
    "# execute and print results of audit on the XML file\n",
    "problematic_street_types = audit(osmfile)\n",
    "pprint.pprint(dict(audit(osmfile)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">There are only twelve instances of abbreviation, and the code below will use the \"*update_name*\" function to change them to a non-abbreviated form. I did find a street abbreviation, \"Ext,\" that wasn't in my street type mapping, so I added it and its pair (Extension) to the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Winthrop Ave => Winthrop Avenue\n",
      "Ardsley => Ardsley\n",
      "2001 Selwyn Ave. => 2001 Selwyn Avenue\n",
      "Blythe Blvd => Blythe Boulevard\n",
      "South Blvd => South Boulevard\n",
      "East Blvd => East Boulevard\n",
      "Iverson Way => Iverson Way\n",
      "Sardis Road North => Sardis Road North\n",
      "Old Nations Ford Rd => Old Nations Ford Road\n",
      "Monroe Rd => Monroe Road\n",
      "Albemarle Rd => Albemarle Road\n",
      "Sardis Road North Suite 4b => Sardis Road North Suite 4b\n",
      "Montford Dr => Montford Drive\n",
      "W Hill St => W Hill Street\n",
      "Harrisonwoods Pl => Harrisonwoods Place\n",
      "Abbey Pl #4 => Abbey Pl #4\n",
      "The Plaza => The Plaza\n",
      "West 4th Street Ext => West 4th Street Extension\n"
     ]
    }
   ],
   "source": [
    "# execute the update_name function and print the mapping of old street name to new, updated street name\n",
    "for st_type, ways in problematic_street_types.items(): \n",
    "    for name in ways:\n",
    "        better_name = update_name(name, mapping)\n",
    "        print (name, \"=>\", better_name) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing and Cleaning <a id='Data Processing and Cleaning'></a>\n",
    "\n",
    ">The code below defines the functions that process the XML data and perform all the cleaning processes described above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## code block to process XML data, unbundle colon-structured values, split the element name and attributes into \n",
    "## dictionaries, validate that the resulting dictionaries are in the correct structure and data type for their \n",
    "## destination SQL tables, and finally, write the dictionaries to CSV files\n",
    "\n",
    "import csv\n",
    "import codecs\n",
    "from collections import defaultdict, OrderedDict\n",
    "import pprint\n",
    "import re\n",
    "import xml.etree.cElementTree as ET\n",
    "import cerberus\n",
    "import schema # note: refers to the schema.py file attached in this directory  \n",
    "\n",
    "OSM_PATH = \"Charlotte_AOI.xml\"\n",
    "\n",
    "# CSV files to write to\n",
    "NODES_PATH = \"nodes.csv\"\n",
    "NODE_TAGS_PATH = \"nodes_tags.csv\"\n",
    "WAYS_PATH = \"ways.csv\"\n",
    "WAY_NODES_PATH = \"ways_nodes.csv\"\n",
    "WAY_TAGS_PATH = \"ways_tags.csv\"\n",
    "\n",
    "# define regular expression to compile, for identifying street type, colon-structured values, and problem characters\n",
    "street_type_re = re.compile(r'\\b\\S+\\.?$', re.IGNORECASE)\n",
    "LOWER_COLON = re.compile(r'^([a-z]|_)+:([a-z]|_)+')\n",
    "PROBLEMCHARS = re.compile(r'[=\\+/&<>;\\'\"\\?%#$@\\,\\. \\t\\r\\n]')\n",
    "\n",
    "# schema to validate dictionaries against\n",
    "SCHEMA = schema.schema\n",
    "\n",
    "# right order of columns for each dict/CSV file\n",
    "NODE_FIELDS = ['id', 'lat', 'lon', 'user', 'uid', 'version', 'changeset', 'timestamp']\n",
    "NODE_TAGS_FIELDS = ['id', 'key', 'value', 'type']\n",
    "WAY_FIELDS = ['id', 'user', 'uid', 'version', 'changeset', 'timestamp'] \n",
    "WAY_TAGS_FIELDS = ['id', 'key', 'value', 'type'] \n",
    "WAY_NODES_FIELDS = ['id', 'node_id', 'position'] \n",
    "       \n",
    "# list of expected street types, used when identifying problematic street types\n",
    "expected = [\"Street\", \"Avenue\", \"Boulevard\", \"Drive\", \"Court\", \"Place\", \"Square\", \"Lane\", \"Road\", \n",
    "            \"Trail\", \"Parkway\", \"Commons\"]\n",
    "\n",
    "# dictionary of desired mapping to use when updating street types\n",
    "mapping = { \"St\": \"Street\",\n",
    "            \"St.\": \"Street\",\n",
    "            \"Ave\": \"Avenue\",\n",
    "            \"Ave.\": \"Avenue\",\n",
    "            \"Blvd\": \"Boulevard\",\n",
    "            \"Blvd.\": \"Boulevard\",\n",
    "            \"Dr\": \"Drive\",\n",
    "            \"Dr.\": \"Drive\",\n",
    "            \"Ext\": \"Extension\",\n",
    "            \"Ext.\": \"Extension\",\n",
    "            \"Ct\": \"Court\",\n",
    "            \"Ct.\": \"Court\",\n",
    "            \"Pl\": \"Place\",\n",
    "            \"Pl.\": \"Place\",\n",
    "            \"Sq\": \"Square\",\n",
    "            \"Sq.\": \"Square\",\n",
    "            \"Ln\": \"Lane\",\n",
    "            \"Ln.\": \"Lane\",\n",
    "            \"Rd\": \"Road\",\n",
    "            \"Rd.\": \"Road\",\n",
    "            \"Trl\": \"Trail\",\n",
    "            \"Trl.\": \"Trail\",\n",
    "            \"Pkwy\": \"Parkway\",\n",
    "            \"Pkwy.\": \"Parkway\",\n",
    "            \"Cmns\": \"Commons\",\n",
    "            \"Cmns.\":\"Commons\"\n",
    "            }\n",
    "\n",
    "# function to process each 'tag' sub-element, extracting the attributes, cleaning problematic street types\n",
    "# and unbundling any colon-structured values\n",
    "def shape_tag(element, tags, mapping, node_tags_fields=NODE_TAGS_FIELDS, problem_chars=PROBLEMCHARS, lower_colon=LOWER_COLON, default_tag_type='regular'): \n",
    "    for elem in element.iter(tag=\"tag\"): \n",
    "        if elem.attrib['k'] == \"addr:street\":\n",
    "            m = street_type_re.search(elem.attrib['v'])\n",
    "            if m:\n",
    "                street_type = m.group()\n",
    "                if street_type in expected:\n",
    "                    continue\n",
    "                else:\n",
    "                    old_street_name = elem.attrib['v']\n",
    "                    street_bits = old_street_name.split(\" \")\n",
    "                    if street_bits[-1] not in mapping:\n",
    "                        continue\n",
    "                    else:\n",
    "                        street_bits[-1] = mapping[street_bits[-1]]\n",
    "                        elem.attrib['v'] = \" \".join(street_bits)\n",
    "    for elem in element.iter(tag=\"tag\"): \n",
    "        tag_dict= {} \n",
    "        for field in node_tags_fields: \n",
    "            tag_dict[field] = \"\" \n",
    "        tag_dict[\"id\"] = element.get(\"id\") \n",
    "        for a, b in elem.items(): \n",
    "            if a == \"k\": \n",
    "                if problem_chars.search(b): \n",
    "                    continue \n",
    "                elif lower_colon.search(b): \n",
    "                    splist = b.split(\":\", maxsplit=1) \n",
    "                    tag_dict[\"key\"] = splist[1] \n",
    "                    tag_dict[\"type\"] = splist[0] \n",
    "                else: \n",
    "                    tag_dict[\"key\"] = b \n",
    "                    tag_dict[\"type\"] = default_tag_type \n",
    "            elif a == \"v\": \n",
    "                    tag_dict[\"value\"] = b \n",
    "        tags.append(tag_dict)\n",
    "\n",
    "# function to process and clean elements, extracting node, way, and nd attributes, and processing tags in the same\n",
    "def shape_element(element, node_attr_fields=NODE_FIELDS, way_attr_fields=WAY_FIELDS, way_node_fields=WAY_NODES_FIELDS,\\\n",
    "                  node_tags_fields=NODE_TAGS_FIELDS, problem_chars=PROBLEMCHARS, lower_colon=LOWER_COLON, default_tag_type='regular'):\n",
    "    \"\"\"Clean and shape node or way XML element to Python dict\"\"\"\n",
    "    node_attribs = {} \n",
    "    way_attribs = {}\n",
    "    way_nodes = []\n",
    "    tags = []  \n",
    "   \n",
    "    if element.tag == \"node\":   \n",
    "        for field in node_attr_fields: \n",
    "            node_attribs[field] = \"\" \n",
    "        for a, b in element.items():\n",
    "            if a in node_attr_fields: \n",
    "                node_attribs[a] = b \n",
    "        shape_tag(element, tags, mapping, node_tags_fields=NODE_TAGS_FIELDS, problem_chars=PROBLEMCHARS, lower_colon=LOWER_COLON, default_tag_type='regular') #run the shape tag function on any tag sub-element\n",
    "                                   \n",
    "    if element.tag == \"way\": \n",
    "        for field in way_attr_fields: \n",
    "            way_attribs[field] = \"\" \n",
    "        for a, b in element.items(): \n",
    "            if a in way_attr_fields: \n",
    "                way_attribs[a]= b \n",
    "        shape_tag(element, tags, mapping, node_tags_fields=NODE_TAGS_FIELDS, problem_chars=PROBLEMCHARS, lower_colon=LOWER_COLON, default_tag_type='regular') #run the shape tag function on any tag sub-element\n",
    "        \n",
    "        for nd in element.iter(tag=\"nd\"): \n",
    "            nd_dict= {} \n",
    "            nd_dict[\"id\"] = element.get(\"id\") \n",
    "            nd_dict[\"node_id\"] = nd.get(\"ref\") \n",
    "            nd_dict[\"position\"] = list(element.iter()).index(nd) \n",
    "            way_nodes.append(nd_dict)\n",
    "    if element.tag == 'node':\n",
    "        return {'node': node_attribs, 'node_tags': tags}\n",
    "    elif element.tag == 'way':\n",
    "        return {'way': way_attribs, 'way_tags': tags, 'way_nodes': way_nodes}\n",
    " \n",
    "\n",
    "# ================================================== #\n",
    "#               Helper Functions                     #\n",
    "# ================================================== #\n",
    "\n",
    "# function to iteratively parse the XML file and yield each element\n",
    "def get_element(osm_file, tags=('node', 'way', 'relation')):\n",
    "    context = ET.iterparse(osm_file, events=('start', 'end'))\n",
    "    _, root = next(context)\n",
    "    for event, elem in context:\n",
    "        if event == 'end' and elem.tag in tags:\n",
    "            yield elem\n",
    "            root.clear()\n",
    "\n",
    "# function to validate if element matches the schema\n",
    "def validate_element(element, validator, schema=SCHEMA):\n",
    "    if validator.validate(element, schema) is not True:\n",
    "        field, errors = next(validator.errors.iteritems())\n",
    "        message_string = \"\\nElement of type '{0}' has the following errors:\\n{1}\"\n",
    "        error_string = pprint.pformat(errors)\n",
    "        \n",
    "        raise Exception(message_string.format(field, error_string)) \n",
    "\n",
    "# extends csv.DictWriter to handle Unicode input\n",
    "class UnicodeDictWriter(csv.DictWriter, object):\n",
    "    def writerows(self, rows):\n",
    "        for row in rows:\n",
    "            self.writerow(row)\n",
    "\n",
    "\n",
    "# ================================================== #\n",
    "#               Main Function                        #\n",
    "# ================================================== #\n",
    "\n",
    "# function to iteratively process each element in XML file and write to CSVs\n",
    "def process_map(file_in, validate):\n",
    "\n",
    "    with codecs.open(NODES_PATH, 'w') as nodes_file, \\\n",
    "            codecs.open(NODE_TAGS_PATH, 'w') as nodes_tags_file, \\\n",
    "            codecs.open(WAYS_PATH, 'w') as ways_file, \\\n",
    "            codecs.open(WAY_NODES_PATH, 'w') as way_nodes_file, \\\n",
    "            codecs.open(WAY_TAGS_PATH, 'w') as way_tags_file:\n",
    "    \n",
    "        nodes_writer = UnicodeDictWriter(nodes_file, NODE_FIELDS)\n",
    "        node_tags_writer = UnicodeDictWriter(nodes_tags_file, NODE_TAGS_FIELDS)\n",
    "        ways_writer = UnicodeDictWriter(ways_file, WAY_FIELDS)\n",
    "        way_nodes_writer = UnicodeDictWriter(way_nodes_file, WAY_NODES_FIELDS)\n",
    "        way_tags_writer = UnicodeDictWriter(way_tags_file, WAY_TAGS_FIELDS)\n",
    "\n",
    "        nodes_writer.writeheader() \n",
    "        node_tags_writer.writeheader() \n",
    "        ways_writer.writeheader() \n",
    "        way_nodes_writer.writeheader() \n",
    "        way_tags_writer.writeheader() \n",
    "\n",
    "        validator = cerberus.Validator() \n",
    "        \n",
    "        \n",
    "        count = 0\n",
    "        for element in get_element(file_in, tags=('node', 'way')):\n",
    "            el = shape_element(element)\n",
    "            if el:\n",
    "                if validate is True:\n",
    "                    validate_element(el, validator)\n",
    "                if element.tag == 'node':\n",
    "                    nodes_writer.writerow(el['node'])\n",
    "                    node_tags_writer.writerows(el['node_tags'])\n",
    "                elif element.tag == 'way':\n",
    "                    ways_writer.writerow(el['way'])\n",
    "                    way_nodes_writer.writerows(el['way_nodes'])\n",
    "                    way_tags_writer.writerows(el['way_tags']) \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    process_map(OSM_PATH, validate=True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data into SQL<a id='Loading Data into SQL'></a>\n",
    ">Now that I have pulled the parent and child information for the way and node elements into CSV files, I now want to take the data in those CSV files and load them into a SQL database for further analysis. To do this, I need to create the SQL database, define the schemas for and create the tables that will populate the database, and then load the CSVs into their respective SQL tables.\n",
    "\n",
    ">I used sqlite3 on my local machine to create the database and tables, and to load the data. After defining the schemas and loading the CSVs one-by-by, several times, I used a link the Udacity Project Details to pull a file made by Stephen Welch that has all the schemas pre-defined.\\* I modified that sql file to create the tables, load the CSVs, delete records with header information, and create a few views, using this code:\n",
    "\n",
    "```SQL\n",
    ".read SQL_code_for_charlotte_osm_db.sql\n",
    "```\n",
    "\n",
    ">Using this code made the process a lot faster and more repeatable! Thanks Alvin Alexander.*\n",
    "\n",
    "\n",
    ">references: \n",
    ">* https://gist.github.com/swwelch/f1144229848b407e0a5d13fcb7fbbd6f\n",
    ">* https://alvinalexander.com/android/sqlite-script-read-execute-how"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration of Data with SQL <a id='Exploration of Data with SQL'></a>\n",
    "<a id='SQL Table Keys'></a>\n",
    "### Table Reference Guide\n",
    ">I've found it very helpful to have a fingertip reference of the structure of my tables nad their primary/foreign keys. Here are the tables for **'charlotte_osm.db'**:\n",
    "\n",
    "nodes:                    \n",
    "    - id (primary key)\n",
    "    - lat \n",
    "    - lon\n",
    "    - user\n",
    "    - uid \n",
    "    - version\n",
    "    - changeset\n",
    "    - timestamp \n",
    "\n",
    "nodes_tags:\n",
    "    - id (foreign key, nodes)\n",
    "    - key\n",
    "    - value\n",
    "    - type \n",
    "    \n",
    "ways:\n",
    "    - id (primary key)\n",
    "    - user\n",
    "    - uid\n",
    "    - version\n",
    "    - changeset\n",
    "    - timestamp\n",
    "    \n",
    "ways_tags:\n",
    "    - id (foreign key, ways)\n",
    "    - key\n",
    "    - value\n",
    "    - type\n",
    "\n",
    "ways_nodes:\n",
    "    - id (foreign key, ways)\n",
    "    - node_id (foreign key, nodes (id))\n",
    "    - position\n",
    "\n",
    "### Questions to Answer:\n",
    ">* Get an overview of the types of elements and their numbers\n",
    "    * Spot check number of: nodes, ways, unique users\n",
    "    \n",
    "    \n",
    ">* How many unique users are there?\n",
    "\n",
    "\n",
    ">* Who are the top users and how much do they contribute compared to the other users?\n",
    "\n",
    "\n",
    ">* What are the main types of nodes, ways, and relations, i.e. what type are they?\n",
    "\n",
    "\n",
    ">* What kind of street information is there now?\n",
    "    * Can you make good composite addresses using more fields?\n",
    "    \n",
    ">* Are there any further issues with street names?\n",
    "    * Can I find other street names in other fields?\n",
    "    \n",
    ">* What fields contain postal codes? \n",
    "    * Do the postal codes look good?\n",
    "    \n",
    ">* Is there more TIGER, GNIS, or normal OSM feature data?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many nodes, ways, and way nodes are there?\n",
    "\n",
    ">How many ways?\n",
    "\n",
    "```SQL\n",
    "SELECT COUNT(*) WaysCount FROM ways;\n",
    "\n",
    "WaysCount\n",
    "----------\n",
    "31370\n",
    "```\n",
    "\n",
    ">How many nodes are there?\n",
    "\n",
    "```SQL\n",
    "SELECT COUNT(*) NodesCount FROM nodes;\n",
    "\n",
    "NodesCount\n",
    "----------\n",
    "270362\n",
    "```\n",
    "\n",
    ">Each way is composed of various nodes, with the sub-element tag of 'Nd.' How of these sub-elements are there for *ways_nodes*?\n",
    "\n",
    "```SQL\n",
    "SELECT COUNT(*) as NdSubEl_Count FROM ways_nodes;\n",
    "\n",
    "NdSubEl_Count\n",
    "-------------\n",
    "306987\n",
    "```\n",
    "\n",
    ">How many tag elements are there for *nodes* and *ways*?\n",
    "\n",
    "```SQL\n",
    "SELECT COUNT(*) as TagsCount FROM (\n",
    "        SELECT *\n",
    "        FROM nodes_tags\n",
    "            UNION ALL \n",
    "        SELECT *\n",
    "        FROM ways_tags) as w_n_tags;\n",
    "\n",
    "TagsCount\n",
    "----------------\n",
    "76340\n",
    "```\n",
    "\n",
    ">There are a lot of nodes, and a lot of descriptive tags and Nd elements. Nodes and their associated Nd sub-elements are by far the most numerous elements. There are not as many ways and relations, which makes sense, as there are bound to be more individual points on the map than there are ways connecting them and relations between them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">How many unique people have edited the map area in question? \n",
    "\n",
    "```SQL\n",
    "SELECT COUNT(DISTINCT(waysnnodes.uid))\n",
    "FROM (SELECT nodes.uid, nodes.user from nodes UNION ALL SELECT ways.uid, ways.user FROM ways) waysnnodes;\n",
    "```\n",
    "\n",
    "Ans: 289"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Let's take a look at a sample of those users. \n",
    "\n",
    "```SQL \n",
    "SELECT DISTINCT waysnnodes.user, waysnnodes.uid\n",
    "FROM (SELECT nodes.uid, nodes.user from nodes UNION ALL SELECT ways.uid, ways.user FROM ways) waysnnodes\n",
    "LIMIT 15\n",
    ";\n",
    "```\n",
    "\n",
    "```SQL\n",
    "user                  uid\n",
    "--------------------  -------------------------\n",
    "davidearl             3582\n",
    "Edward                364\n",
    "suodrak               630540\n",
    "fbthies               939079\n",
    "nyuriks               339581\n",
    "woodpeck_repair       145231\n",
    "pratikyadav           2905914\n",
    "woodpeck_fixbot       147510\n",
    "bdiscoe               402624\n",
    "Munchabunch           414318\n",
    "rickmastfan67         252811\n",
    "Omnific               1408522\n",
    "KristenK              1494110\n",
    "SEVEN                 127989\n",
    "Dami_Tn               2012449\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Let's see which of those users are the top contributors, based on the number of edits. \n",
    "\n",
    ">First, I'll create a view for users and the count of their contributions.\n",
    "\n",
    "```SQL\n",
    "CREATE VIEW UserContributionTotals AS\n",
    "SELECT COUNT(waysnnodes.uid) as UserTotals, waysnnodes.user\n",
    "FROM (\n",
    "    SELECT nodes.uid, nodes.user \n",
    "    FROM nodes \n",
    "    UNION ALL \n",
    "    SELECT ways.uid, ways.user \n",
    "    FROM ways) waysnnodes\n",
    "GROUP BY waysnnodes.uid\n",
    "ORDER BY COUNT(waysnnodes.uid) DESC\n",
    ";\n",
    "```\n",
    ">Then query the view for the top 20 contributors.\n",
    "\n",
    "```SQL\n",
    "SELECT UserTotals, user\n",
    "FROM (SELECT * FROM UserContributionTotals LIMIT 20)\n",
    ";\n",
    "```\n",
    ">These are the top 20 contributors, by number of edits. As you can see, the distribution of top contributors is very skewed, with the top three contributing an order of magnitude more than any others.\n",
    "\n",
    "```SQL\n",
    "UserTotals            user\n",
    "--------------------  -------------------------\n",
    "165243                Omnific\n",
    "47550                 woodpeck_fixbot\n",
    "21933                 jumbanho\n",
    "7222                  HeyYoJimbo\n",
    "5349                  bdiscoe\n",
    "5025                  Yoshinion\n",
    "4466                  Becker_MN_Import_Acc\n",
    "3220                  WashuOtaku\n",
    "2954                  Fortepc\n",
    "2694                  Bancheee\n",
    "2495                  tobin\n",
    "1815                  maxerickson\n",
    "1803                  Brian F\n",
    "1787                  bot-mode\n",
    "1748                  woodpeck_repair\n",
    "1422                  andrewpmk\n",
    "1334                  Jruze\n",
    "1283                  rickmastfan67\n",
    "1223                  emacsen_dwg\n",
    "1211                  botdidier2020\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> How many total user contributions are there for this area?\n",
    "\n",
    "```SQL\n",
    "SELECT SUM(UserTotals)\n",
    "FROM (SELECT * FROM UserContributionTotals)\n",
    ";\n",
    "```\n",
    "> Ans: 301732\n",
    "\n",
    "> How many contributions did the top three users make?\n",
    "\n",
    "```SQL\n",
    "SELECT SUM(UserTotals)\n",
    "FROM (SELECT * FROM UserContributionTotals LIMIT 3)\n",
    ";\n",
    "```\n",
    "\n",
    "> Ans: 234726 contributions, or ~78% of the contributions, which suggests a Pareto relationship between users and mapping contributions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> What are the main types of nodes and ways, i.e. what do those elements represent on the map?\n",
    "\n",
    "> Before examining this information further, I will combine the *nodes_tags* and *ways_tags* tables, which have the same <a href='#SQL Table Keys'>structure</a>, in order to get a more complete picture of what these data sets hold. \n",
    "\n",
    "```SQL\n",
    "CREATE VIEW nodeANDway_tags AS\n",
    "        SELECT *\n",
    "        FROM nodes_tags\n",
    "            UNION ALL \n",
    "        SELECT *\n",
    "        FROM ways_tags\n",
    ";\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> What categories of tag 'type' are there?\n",
    "\n",
    "```SQL\n",
    "SELECT type FROM nodeANDway_tags GROUP BY type;\n",
    "\n",
    "type\n",
    "-------------------------\n",
    "addr\n",
    "area\n",
    "brand\n",
    "building\n",
    "census\n",
    "communication\n",
    "contact\n",
    "cost\n",
    "cycleway\n",
    "demolished\n",
    "destination\n",
    "diet\n",
    "gnis\n",
    "healthcare\n",
    "hgv\n",
    "internet_access\n",
    "is_in\n",
    "lanes\n",
    "maxspeed\n",
    "mtb\n",
    "name\n",
    "note\n",
    "payment\n",
    "railway\n",
    "regular\n",
    "roof\n",
    "service\n",
    "source\n",
    "tiger\n",
    "toilets\n",
    "tower\n",
    "turn\n",
    "wheelchair\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Originally, the type field was meant to unbundle colon-bundled address information like \"addr:street,\" \"addr:house,\" etc... It appears that many more pieces of colon-bundled information got swept up by the *\"shape_tag\"* helper function of the *\"shape_element\"* function.\n",
    "\n",
    "> Let's see what 'keys' were captured for the addr type.\n",
    "\n",
    "```SQL\n",
    "SELECT type, key FROM nodeANDway_tags WHERE type == 'addr' GROUP BY key;\n",
    "\n",
    "type        key\n",
    "----------  --------------------\n",
    "addr        city\n",
    "addr        country\n",
    "addr        door\n",
    "addr        floor\n",
    "addr        housename\n",
    "addr        housenumber\n",
    "addr        housenumber_1\n",
    "addr        place\n",
    "addr        postcode\n",
    "addr        state\n",
    "addr        street\n",
    "addr        street_1\n",
    "addr        unit\n",
    "```\n",
    "\n",
    "> There are 12 different keys tied to the 'addr' type. The values associated with each of these keys will be helpful in compiling complete address information for a node. \n",
    "\n",
    "> The code below will show the records pertaining to one particular unit of address information.\n",
    "\n",
    "```SQL\n",
    "SELECT * FROM nodeANDway_tags WHERE type == addr ORDER BY id LIMIT 5;\n",
    "\n",
    "id          key                   value                 type\n",
    "----------  --------------------  --------------------  ----------\n",
    "38619400    city                  Charlotte             addr\n",
    "38619400    housenumber           2365                  addr\n",
    "38619400    postcode              28266                 addr\n",
    "38619400    state                 NC                    addr\n",
    "38619400    street                Carmel Road           addr\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> #### What values are there for:\n",
    "> * state?\n",
    "> * postcode?\n",
    "> * house number?\n",
    "> * city?\n",
    "> * country?\n",
    "\n",
    "```SQL\n",
    "SELECT COUNT(*), key, value, type\n",
    "FROM nodeANDway_tags\n",
    "WHERE key LIKE '%state'\n",
    "GROUP BY value\n",
    ";\n",
    "\n",
    "COUNT(*)              key                        value       type\n",
    "--------------------  -------------------------  ----------  ----------\n",
    "222                   state                      NC          addr\n",
    "```\n",
    "\n",
    "> I wouldn't expect there to be other states than NC, but it was worth checking. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> What range of values are there for postal codes? Are there any obvious errors?\n",
    "\n",
    "```SQL\n",
    "SELECT COUNT(*), key, value, type\n",
    "FROM nodeANDway_tags\n",
    "WHERE key LIKE '%postcode'\n",
    "GROUP BY value\n",
    "ORDER BY value \n",
    ";\n",
    "\n",
    "COUNT(*)    key         value       type\n",
    "----------  ----------  ----------  ----------\n",
    "5           postcode    28023       addr\n",
    "42          postcode    28202       addr\n",
    "62          postcode    28203       addr\n",
    "17          postcode    28204       addr\n",
    "31          postcode    28205       addr\n",
    "12          postcode    28207       addr\n",
    "7           postcode    28208       addr\n",
    "55          postcode    28209       addr\n",
    "22          postcode    28210       addr\n",
    "14          postcode    28211       addr\n",
    "1           postcode    28211-4411  addr\n",
    "4           postcode    28212       addr\n",
    "8           postcode    28217       addr\n",
    "5           postcode    28226       addr\n",
    "2           postcode    28227       addr\n",
    "1           postcode    28244       addr\n",
    "1           postcode    28266       addr\n",
    "10          postcode    28270       addr\n",
    "1           postcode    28273       addr\n",
    "1           postcode    28274       addr\n",
    "1           postcode    48009       addr\n",
    "1           postcode    NC          addr\n",
    "1           postcode    NC 28202    addr\n",
    "1           postcode    NC28209     addr\n",
    "```\n",
    "\n",
    ">There are a few postal codes that stand out: 28023, 48009, and, obviously, 28211-4411, NC, NC 28202, and NC28209. The last four are clearly errors or non-uniform data. Regarding the first two, 28023 is probably a typing error, as it is a zip code for a <a href='https://www.google.com/maps/place/China+Grove,+NC+28023/@35.5746279,-80.7341066,11z/data=!3m1!4b1!4m5!3m4!1s0x8853f7bf66dda875:0xb46ea0adc083b48f!8m2!3d35.5805091!4d-80.5993854'>town</a> that is an hour away, and 28203 is a zip code close to downtown Charlotte. 48009 is definitely an error, since it is a postal code for <a href='https://www.google.com/maps/place/Birmingham,+MI+48009/@42.5456942,-83.2342796,14z/data=!3m1!4b1!4m5!3m4!1s0x8824c7a8176abb85:0x8ed1a286ce141f4a!8m2!3d42.5410436!4d-83.2131206'>Birmingham, Michigan</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> How about the values for housenumber?\n",
    "\n",
    "```SQL\n",
    "SELECT COUNT(*), key, value, type\n",
    "FROM nodeANDway_tags\n",
    "WHERE key LIKE '%housenumber'\n",
    "GROUP BY value\n",
    "ORDER BY value \n",
    ";\n",
    "```\n",
    "\n",
    "> Nothing remarkable in the results of that query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Cities, in the city of Charlotte?\n",
    "\n",
    "```SQL\n",
    "SELECT COUNT(*), key, value, type\n",
    "FROM nodeANDway_tags\n",
    "WHERE key LIKE '%city'\n",
    "GROUP BY value\n",
    "ORDER BY value \n",
    ";\n",
    "\n",
    "COUNT(*)    key         value                 type\n",
    "----------  ----------  --------------------  ----------\n",
    "2           capacity    4                     regular\n",
    "1           capacity    5                     regular\n",
    "1           capacity    55                    regular\n",
    "1           city        Charlote              addr\n",
    "306         city        Charlotte             addr\n",
    "1           city        Charlotte, NC         addr\n",
    "```\n",
    "\n",
    "> Just a spelling error and some extra information. Only one record with each error!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Last one, any unusual countries?\n",
    "\n",
    "```SQL\n",
    "SELECT COUNT(*), key, value, type\n",
    "FROM nodeANDway_tags\n",
    "WHERE key LIKE '%country'\n",
    "GROUP BY value\n",
    "ORDER BY value \n",
    ";\n",
    "\n",
    "COUNT(*)    key         value                      type\n",
    "----------  ----------  -------------------------  ----------\n",
    "6           country     US                         addr\n",
    "2           country     USA                        addr\n",
    "```\n",
    "\n",
    "> All US or USA. It would probably be worth making those entries uniform.\n",
    "\n",
    "> *Reference:*\n",
    "\n",
    "> * *SQL 'LIKE '%example' parameter, Udacity SQL Sample Project, https://gist.github.com/carlward/54ec1c91b62a5f911c42#file-sample_project-md*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Worthy Options for Further Cleaning <a id='Worthy Options for Further Cleaning'></a>\n",
    "> There were a few data cleanliness issues that I did not address, and which could be good opportunities for further work. For example, I focused on cleaning address information of the OSM feature type, but there are also many records from the Tiger dataset. For example, see this 'highway' record:\n",
    "\n",
    "```XML\n",
    "<way id=\"16668607\" version=\"7\" timestamp=\"2014-08-14T13:49:20Z\" changeset=\"24746007\" uid=\"105454\" user=\"hopfavogl\">\n",
    "    <nd ref=\"172274468\"/>\n",
    "    <nd ref=\"3016461626\"/>\n",
    "    <nd ref=\"172274470\"/>\n",
    "    <nd ref=\"2401902303\"/>\n",
    "    <nd ref=\"172274476\"/>\n",
    "    <tag k=\"highway\" v=\"residential\"/>\n",
    "    <tag k=\"name\" v=\"Piedmont Street\"/>\n",
    "    <tag k=\"tiger:cfcc\" v=\"A41\"/>\n",
    "    <tag k=\"tiger:county\" v=\"Mecklenburg, NC\"/>\n",
    "    <tag k=\"tiger:name_base\" v=\"Piedmont\"/>\n",
    "    <tag k=\"tiger:name_type\" v=\"St\"/>\n",
    "    <tag k=\"tiger:reviewed\" v=\"no\"/>\n",
    "    <tag k=\"tiger:zip_left\" v=\"28204\"/>\n",
    "    <tag k=\"tiger:zip_right\" v=\"28204\"/>\n",
    "  </way>\n",
    "```\n",
    "\n",
    "> In fact, if I were to focus on more data cleaning and/or compiling detailed addresses for features, it would be nice to know which source of address data has more features, the OSM, GNIS, or Tiger data types. \n",
    "\n",
    "```SQL\n",
    "SELECT type, COUNT(DISTINCT(id)) as Num_Features\n",
    "FROM nodeANDway_tags\n",
    "WHERE type = \"gnis\" OR type = \"tiger\" OR type = \"addr\"\n",
    "GROUP BY type\n",
    "ORDER BY COUNT(*) DESC\n",
    ";\n",
    "\n",
    "type        Num_Features\n",
    "----------  ---------------\n",
    "tiger       3105\n",
    "addr        397\n",
    "gnis        201\n",
    "```\n",
    "\n",
    "> As it turns out, the tiger data has address information for the most features, followed the OSM (addr) type, and then the GNIS type. For the sake of efficiency, it would definitely be worth focusing on the Tiger records. Thus, to more thoroughly clean street information, it would be a very good idea to expand the \"*is_street_name*\" function to include these records. This would not be a very difficult process.\n",
    "\n",
    "\n",
    "> Another issue I noticed is in node tags with the key '*is_in*'. They all have the value '*Mecklenburg,North Carolina,N.C.,NC,USA*,' which clearly needs to be cleaned up. See the example element below.\n",
    "\n",
    ">To fix this issue, one could simply alter four functions: '*is_street_name*,' '*audit_street_type*, '*audit*', and '*update_name*'. Instead of looking for and updating a street type, you would modify the functions to look for 'is_in' attributes and change their values to a clean version of the duplicate-ridden value. \n",
    "\n",
    "```XML\n",
    "  <node id=\"153902091\" lat=\"35.2136142\" lon=\"-80.8257289\" version=\"4\" timestamp=\"2017-09-24T08:41:42Z\" changeset=\"52322768\" uid=\"364\" user=\"Edward\">\n",
    "    <tag k=\"ele\" v=\"209\"/>\n",
    "    <tag k=\"gnis:Class\" v=\"Populated Place\"/>\n",
    "    <tag k=\"gnis:County\" v=\"Mecklenburg\"/>\n",
    "    <tag k=\"gnis:County_num\" v=\"119\"/>\n",
    "    <tag k=\"gnis:ST_alpha\" v=\"NC\"/>\n",
    "    <tag k=\"gnis:ST_num\" v=\"37\"/>\n",
    "    <tag k=\"gnis:id\" v=\"1001262\"/>\n",
    "    <tag k=\"import_uuid\" v=\"bb7269ee-502a-5391-8056-e3ce0e66489c\"/>\n",
    "    <tag k=\"is_in\" v=\"Mecklenburg,North Carolina,N.C.,NC,USA\"/>\n",
    "    <tag k=\"name\" v=\"Elizabeth\"/>\n",
    "    <tag k=\"place\" v=\"hamlet\"/>\n",
    "    <tag k=\"wikidata\" v=\"Q5362242\"/>\n",
    "  </node>\n",
    "```\n",
    "\n",
    "```SQL\n",
    "SELECT COUNT(*), key, value, type FROM nodeANDway_tags WHERE key == 'is_in';\n",
    "\n",
    "COUNT(*)    key         value                                     type\n",
    "----------  ----------  ----------------------------------------  ----------\n",
    "85          is_in       Mecklenburg,North Carolina,N.C.,NC,USA    regular\n",
    "```\n",
    "\n",
    ">Since there are 85 records with this issue, it would definitely be worth doing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
